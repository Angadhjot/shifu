# zookeeperServers is used for distributed training 
zookeeperServers=

# hadoopNumParallel is for parallelity for when using mapreduce mode
# deprecated: there is auto parallel setting in all pig scripts to help user no need to set this number
#hadoopNumParallel=40

# hadoopJobQueue specifies the hadoop job queue
hadoopJobQueue=default

# localNumParallel is for parallelity for when using local mode
localNumParallel=6

# how many records per message
recordCntPerMessage=100000

# fix a bug on hdp 2.4.1 by default mapreduce.job.max.split.locations is 10
mapreduce.job.max.split.locations=100

# disable map output compress because some issue for snappy config in our clusters
mapreduce.map.output.compress=false

# how to control how many mappers when training models
# If there are too many small files after normalization, set it to true
# It will try to merge multi small files in mapper and reduce the number of mappers
# guagua.split.combinable=true

# It will control the max combined split size for each mapper.
# Sometimes mapper will timeout for loading data, if there are too much data for mapper
# This option can limit the records for each mapper
# guagua.split.maxCombinedSplitSize=256000000